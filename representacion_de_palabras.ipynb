{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representación de palabras mediante espacios vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Créditos\n",
    "\n",
    "Estas clases y material están fuertemente inspirado en los cursos de Stanford [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/) y [CS224u: Natural Language Understanding](http://web.stanford.edu/class/cs224u/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "__author__ = \"Cristian Cardellino\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contenidos\n",
    "\n",
    "1. [Representación de palabras](#Representación-de-palabras)\n",
    "1. [Representación distribuída de palabras](#Representación-distribuída-de-palabras)\n",
    "1. [Diseño de matrices](#Diseño-de-matrices)\n",
    "1. [Comparación de vectores](#Comparación-de-vectores)\n",
    "1. [Métodos de reponderación](#Métodos-de-reponderación)\n",
    "1. [Información de subpalabras](#Información-de-subpalabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import vsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representación de palabras\n",
    "\n",
    "1. [Características de las palabras](#Características-de-las-palabras)\n",
    "1. [Representación mediante taxonomías](#Representación-mediante-taxonomías)\n",
    "1. [Representación discreta](#Representación-discreta)\n",
    "1. [Ejemplo de representación discreta](#Ejemplo-de-representación-discreta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Características de las palabras\n",
    "\n",
    "- Son la **unidad básica** en cualquier tarea de procesamiento de lenguaje natural (PLN).\n",
    "- Cualquier modelo de PLN requiere como entrada la **representación de una palabra**.\n",
    "- Gran parte del trabajo de PLN solía representar las palabras como **símbolos atómicos**.\n",
    "- Las nociones de **similitu y distancia** entre palabras son cruciales para tareas de PLN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representación mediante taxonomías\n",
    "\n",
    "- El **sentido** de una palabras es la idea que una palabra (o frase) representa.\n",
    "- Se puede representar mediante recursos del estilo de las **taxonomías**.\n",
    "- Ofrecen **información rica y estructurada**.\n",
    "    - WordNet asigna relaciones de **hiperonimia/hiponimia** (relación *es-un*), y **sinonimia**.\n",
    "    - FrameNet establece la **estructura** mediante un marco sintáctico y semántico de las palabras.\n",
    "- Son **difíciles y caros de obtener y actualizar** (requiren anotación humana)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog's hypernyms:\n",
      "Synset('canine.n.02')\n",
      "Synset('domestic_animal.n.01')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn  # Run nltk.download(\"wordnet\") first\n",
    "\n",
    "dog = wn.synset(\"dog.n.01\")\n",
    "\n",
    "print(\"Dog's hypernyms:\")\n",
    "for hypernym in dog.hypernyms():\n",
    "    print(hypernym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representación discreta\n",
    "\n",
    "- Para aplicar medidas de similitud y distancia (e.g. euclídea, coseno), utilizamos **vectores que representen palabras**.\n",
    "- Primera aproximación: **vectores *one-hot***.\n",
    "- Para cada palabra del vocabulario, tenemos un vector.\n",
    "- En términos de espacio vectorial, el vector tiene un **1** en una de sus dimensiones y __0s__ en todas las demás.\n",
    "- **Dimensionalidad muy alta** (el inglés tiene un estimado de 13 millones de palabras).\n",
    "- Los vectores son **ralos** (o esparsos), tienen muchos ceros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de representación discreta\n",
    "\n",
    "Dado un corpus:\n",
    "\n",
    "- \"El fallo fue decisivo\"\n",
    "- \"La corte rectificará el fallo\"\n",
    "\n",
    "Tenemos un vocabulario $V = \\{corte, decisivo, el, fallo, fue, la, rectificará\\}$.\n",
    "\n",
    "Codificamos un número $|V| = 7$ de vectores para cada palabra $w^{(i)} \\in \\mathbb{R}^{|V|}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de representación discreta\n",
    "\n",
    "$$\n",
    "    w^{corte} =\n",
    "    \\begin{bmatrix}\n",
    "       1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{decisivo} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{el} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{fallo} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "$$\n",
    "\n",
    "$$\n",
    "    w^{fue} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix}\n",
    "    w^{la} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{rectificará} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"El fallo fue decisivo\", \n",
    "    \"La corte rectificará el fallo\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [\n",
    "    [word.lower() for word in doc.split()] for doc in corpus\n",
    "]\n",
    "\n",
    "vocabulary = sorted(set([\n",
    "    word for doc in tokenized_corpus for word in doc\n",
    "]))\n",
    "\n",
    "one_hot_vectors = np.eye(len(vocabulary), dtype=np.int)\n",
    "\n",
    "for doc in tokenized_corpus:\n",
    "    print(np.array([\n",
    "        one_hot_vectors[vocabulary.index(word), :] for word in doc\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representación distribuída de palabras\n",
    "\n",
    "1. [Concepto](#Concepto)\n",
    "1. [Motivación](#Motivación)\n",
    "1. [Hipótesis para guiarse](#Hipótesis-para-guiarse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Concepto\n",
    "\n",
    "- Idea: Representar una palabra **mediante sus vecinos**.\n",
    "    - Es una de las **ideas más importantes** en PLN moderno.\n",
    "\n",
    "#### Ejemplo: \n",
    "\n",
    "- \"_El gobierno rescató a los_ **bancos** _ante la crisis de crédito_\".\n",
    "- \"_La municipalidad reparará los_ **bancos** _de la plaza_\".\n",
    "\n",
    "En estos ejemplos, las palabras que están en _cursiva_ representarán las palabras en **negrita**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Motivación\n",
    "\n",
    "- La representación distribuída nos da pie a pensar como los vectores **codifican el significado de las unidades linguísticas** (palabras, oraciones, documentos, etc.).\n",
    "- Son la base de los **modelos de espacio vectorial**.\n",
    "- Piezas fundamentales en modelos para **Procesamiento y Comprensión de Lenguaje Natural**.\n",
    "- Estas representaciones pueden ser usadas, entre otros, para:\n",
    "    - Entender y modelar fenómenos sociales y lingüísticos.\n",
    "    - Entrada para modelos de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hipótesis para guiarse\n",
    "\n",
    "- \"You shall know a word by the company it keeps\" (Firth, 1957).\n",
    "- \"The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously\" (Firth, 1957).\n",
    "- \"The meaning of a word is its use in the language\" (Wittgenstein, 1957).\n",
    "- \"Distributional statements can cover all of the material of a language without requiring support from other types of information\" (Harris, 1954)\n",
    "- \"If units of text have similar vectors in a text frequency matrix, then they tend to have similar meanings.\" (Turney & Pantel, 2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diseño de matrices\n",
    "\n",
    "1. [Objetivos del diseño de matrices](#Objetivos-del-diseño-de-matrices)\n",
    "1. [Matriz de co-ocurrencia de palabras](#Matriz-de-co-ocurrencia-de-palabras)\n",
    "1. [Matriz de documentos](#Matriz-de-documentos)\n",
    "1. [Ventana y ponderación](#Ventana-y-ponderación)\n",
    "1. [Más diseños de matrices](#Más-diseños-de-matrices)\n",
    "1. [Consideraciones](#Consideraciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Objetivos del diseño de matrices\n",
    "\n",
    "- Buscamos representar palabras de manera matemática.\n",
    "    - Idea: Mediante una matriz (vista como un conjunto de vectores)\n",
    "- Hay que definir la construcción de una matriz.\n",
    "    - Muchas opciones de diseño.\n",
    "    - Distinto impacto tendrán las decisiones tomadas respecto al modelado del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matriz de co-ocurrencia de palabras\n",
    "\n",
    "- Cada palabra se representa por las palabras a su alrededor (dada una ventana).\n",
    "- La matriz es simétrica (independiente de la posición de las palabras).\n",
    "    - Esto se conoce como \"bolsa de palabras\" (o _bag of words_)\n",
    "- Dado un vocabulario $V$:\n",
    "    - Matriz de co-ocurrencias $X \\in \\mathbb{R}^{|V|\\times|V|}$\n",
    "    - $X_{ij}$ representa la co-ocurrencia entre la palabras $w^{(i)}$ y $w^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matriz de co-ocurrencia de palabras\n",
    "\n",
    "<img src=\"img/word-word-matrix.png\" width=\"90%\"/>\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matriz de documentos\n",
    "\n",
    "- Cada documento se representa por las palabras que lo conforman.\n",
    "- Se usa un modelo de bolsa de palabras: no importa la posición.\n",
    "- Dadas $N$ palabras en un corpus de $M$ documentos:\n",
    "    - Matriz de documentos $X \\in \\mathbb{R}^{N \\times M}$\n",
    "    - $X_{ij}$ representa la ocurrencia de la palabra $w^{(i)}$ en el documento $d^{(j)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matriz de documentos\n",
    "\n",
    "<img src=\"img/term-document-matrix.png\" width=\"90%\"/>\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ventana y ponderación\n",
    "\n",
    "- La ventana define la **cantidad de palabras en el contexto** a considerar.\n",
    "- La ponderación (*scaling*) le da un **peso a la palabra** en la suma total.\n",
    "    - Igual peso para todas las palabras, i.e. bolsa de palabras.\n",
    "    - Peso distinto de acuerdo a la distancia, e.g. decaimiento fraccional.\n",
    "    - Peso binario, i.e. la palabra está o no.\n",
    "\n",
    "<img src=\"img/window-scaling.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ventana y ponderación\n",
    "\n",
    "- Ventanas más grandes y con peso igual (_flat_) capturan **información semántica**.\n",
    "    - Las matrices de documentos dan idea de la temática del documento.\n",
    "- Ventanas más cortas y con pesos capturan **información sintáctica** (colocacional).\n",
    "    - Las matrices de co-ocurrencias de palabras son de afinidad de palabras.\n",
    "- El límite puede definirse más allá de una ventana fija.\n",
    "    - Utilizar oraciones, párrafos o documentos tendrá consecuencias distintas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "infoleg5 = pd.read_csv(\"./data/infoleg_5window_scaled.csv.gz\", index_col=0)\n",
    "\n",
    "infoleg20 = pd.read_csv(\"./data/infoleg_20window_flat.csv.gz\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Más diseños de matrices\n",
    "\n",
    "- palabra $\\times$ relación de dependencia\n",
    "- palabra $\\times$ contexto sintáctico\n",
    "- adjetivo $\\times$ sustantivo modificado\n",
    "- palabra $\\times$ consulta de búsqueda\n",
    "- persona $\\times$ producto\n",
    "- palabra $\\times$ persona\n",
    "- verbo $\\times$ sujeto $\\times$ objeto\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Consideraciones\n",
    "\n",
    "- Las matrices **incrementan con el vocabulario**.\n",
    "    - Definir límites en vocabulario o bien lidiar con modelos muy grandes.\n",
    "    - Por la ley de Zipf (Zipf, 1949), la lista de palabras con escasa aparición se vuelve muy grande.\n",
    "- La elección de la ventana afecta en la obtención de **matrices más o menos dispersas**.\n",
    "- Representar distintas unidades lingüísticas deriva en **más dispersión** (ver el ejemplo de word-word vs term-document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparación de vectores \n",
    "\n",
    "1. [Objetivos de comparar vectores](#Objetivos-de-comparar-vectores)\n",
    "1. [Ejemplo de juguete](#Ejemplo-de-juguete)\n",
    "1. [Distancia euclídea](#Distancia-euclídea)\n",
    "1. [Normalización del vector](#Normalización-del-vector)\n",
    "1. [Distancia coseno](#Distancia-coseno)\n",
    "1. [Medidas basadas en coincidencias](#Medidas-basadas-en-coincidencias)\n",
    "1. [Propiedades de una distancia](#Propiedades-de-una-distancia)\n",
    "1. [Otras medidas](#Otras-medidas)\n",
    "1. [Exploración de vecinos](#Exploración-de-vecinos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Objetivos de comparar vectores\n",
    "\n",
    "- Está en el núcleo de nuestro análisis.\n",
    "- Generalmente buscamos **medir distancias** entre vectores.\n",
    "    - La idea es que palabras relacionadas estén cercanas en el espacio vectorial construído.\n",
    "- El módulo [scipy.spatial.distance](http://docs.scipy.org/doc/scipy-0.14.0/reference/spatial.distance.html) ofrece una gran cantidad de métricas para comparar vectores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de juguete\n",
    "\n",
    "<img src=\"img/vector-toy-example.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de juguete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dx    dy\n",
       "A   2.0   4.0\n",
       "B  10.0  15.0\n",
       "C  14.0  10.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ABC = pd.DataFrame([\n",
    "    [ 2.0,  4.0], \n",
    "    [10.0, 15.0], \n",
    "    [14.0, 10.0]],\n",
    "    index=['A', 'B', 'C'],\n",
    "    columns=['dx', 'dy'])\n",
    "\n",
    "ABC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de juguete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_ABC(df):\n",
    "    ax = df.plot.scatter(x='dx', y='dy', marker='.', legend=False)\n",
    "    m = df.values.max(axis=None)\n",
    "    ax.set_xlim([0, m*1.2])\n",
    "    ax.set_ylim([0, m*1.2])\n",
    "    for label, row in df.iterrows():\n",
    "        ax.text(row['dx'], row['dy'], label)\n",
    "\n",
    "plot_ABC(ABC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distancia euclídea\n",
    "\n",
    "- Es la **distancia** más básica e intuitiva entre dos vectores.\n",
    "- La distancia euclídea entre dos vectores $u$ y $v$ de dimensión $n$ se define como:\n",
    "\n",
    "$$\\textbf{euclidean}(u, v) = \\sqrt{\\sum_{i=1}^{n}|u_{i} - v_{i}|^{2}}$$\n",
    "\n",
    "- En dos dimensiones, se corresponde con la longitud de la línea más directa entre dos puntos.\n",
    "\n",
    "<img src=\"img/euclidean-distance.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distancia euclídea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean(A, B) =   13.60\n",
      "euclidean(B, C) =    6.40\n"
     ]
    }
   ],
   "source": [
    "def abc_comparisons(df, distfunc):\n",
    "    for a, b in (('A', 'B'), ('B', 'C')):\n",
    "        dist = distfunc(df.loc[a], df.loc[b])\n",
    "        print(f\"{distfunc.__name__}({a}, {b}) = {dist:7.02f}\")\n",
    "\n",
    "abc_comparisons(ABC, vsm.euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Suponiendo que estos vectores fueran de palabras, hay algunos aspectos extraños:\n",
    "    - Notar que las distribuciones de B y C son opuestas (e.g. \"bueno\" vs. \"malo\").\n",
    "    - A y B están más alineadas en cambio, salvo por las frecuencias (e.g. \"bueno\" vs. \"excelente\").\n",
    "- La distancia euclídea está **sesgada por tamaño**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalización del vector\n",
    "\n",
    "- Buscamos quitar el sesgo. Para eso **normalizamos el vector de acuerdo a su longitud**.\n",
    "- Definimos la **norma L2** de un vector:\n",
    "\n",
    "$$\\|u\\|_{2} = \\sqrt{\\sum_{i=1}^{n} u_{i}^{2}}$$\n",
    "\n",
    "- Luego, la normalización de $u$ se consigue dividiendo por su longitud (que es escalar):\n",
    "\n",
    "$$\\left[\\frac{u_{1}}{\\|u\\|_{2}}, \\frac{u_{2}}{\\|u\\|_{2}}, \\ldots, \\frac{u_{n}}{\\|u\\|_{2}}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalización del vector\n",
    "\n",
    "<img src=\"img/length-normalization.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalización del vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ABC_normed = ABC.apply(vsm.length_norm, axis=1)\n",
    "plot_ABC(ABC_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean(A, B) =    0.12\n",
      "euclidean(B, C) =    0.36\n"
     ]
    }
   ],
   "source": [
    "abc_comparisons(ABC_normed, vsm.euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distancia coseno\n",
    "\n",
    "- La **distancia coseno toma toda la longitud en cuenta**.\n",
    "- La distancia coseno entre dos vectores $u$ y $v$ de dimensión $n$ se define como:\n",
    "\n",
    "$$\\textbf{cosine}(u, v) = 1 - \\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|_{2} \\cdot \\|v\\|_{2}}$$\n",
    "\n",
    "- La similitud coseno (el término que se está restando), **mide el ángulo entre dos vectores**.\n",
    "- Es **equivalente** a tomar la distancia euclídea de los vectores normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine(A, B) =    0.01\n",
      "cosine(B, C) =    0.07\n"
     ]
    }
   ],
   "source": [
    "abc_comparisons(ABC, vsm.cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distancia coseno\n",
    "\n",
    "<img src=\"img/cosine-distance-1.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distancia coseno\n",
    "\n",
    "<img src=\"img/cosine-distance-2.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Medidas basadas en coincidencias\n",
    "\n",
    "- El método básico basado en coincidencia crea un vector que mide la cantidad de coincidencias entre dos vectores:\n",
    "\n",
    "$$\\textbf{matching}(u, v) = \\sum_{i=1}^{n} \\min(u_{i}, v_{i})$$\n",
    "\n",
    "- El [__coeficiente de Jaccard__](https://en.wikipedia.org/wiki/Jaccard_index) es una manera de normalizar el método de coincidencias.\n",
    "\n",
    "$$\\textbf{jaccard}(u, v) = 1 - \\frac{\\textbf{matching}(u, v)}{\\sum_{i=1}^{n} \\max(u_{i}, v_{i})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard(A, B) =    0.76\n",
      "jaccard(B, C) =    0.31\n"
     ]
    }
   ],
   "source": [
    "abc_comparisons(ABC, vsm.jaccard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Propiedades de una distancia\n",
    "\n",
    "Para que una medida califique como distancia, debe cumplir algunos requisitos.\n",
    "\n",
    "- Simétrica: $d(u, v) = d(v, u)$.\n",
    "- Asignar 0 a vectores idénticos: $d(v, v) = 0$\n",
    "- Satisfacer la **desigualdad triangular**: $d(u, w) \\leq d(u,v) + d(v, w)$\n",
    "    - La distancia coseno, definida arriba, no satisface esta última (hay otras maneras de definirla)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Otras medidas\n",
    "\n",
    "Muchas otras medidas existen con ligeras variaciones. La mayoría se encuentran implementadas en [scipy](https://www.scipy.org/).\n",
    "\n",
    "- [Coeficiente de Dice](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)\n",
    "- [Coeficiente de superposición](https://en.wikipedia.org/wiki/Overlap_coefficient)\n",
    "- [KL Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "    - Symmetric KL\n",
    "    - KL-divergence with Skew\n",
    "    - [Jensen-Shannon](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\n",
    "\n",
    "De estas, sólo Jensen-Sahnnon es una métrica de distancia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploración de vecinos\n",
    "\n",
    "- La función `vsm.neighbors` es útil para investigación.\n",
    "- Para una palabra dada `w`, hace un ranking de todas las palabras en el vocabulario de acuerdo a su distancia con `w`, medida por `distfunc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "senado        0.000000\n",
       "cámara        0.203252\n",
       "controlar     0.215018\n",
       "supervisar    0.219421\n",
       "deberes       0.226206\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsm.neighbors(\"senado\", infoleg5, distfunc=vsm.cosine).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "senado             0.000000\n",
       "o.              4868.879170\n",
       "concordantes    4883.316777\n",
       "administrar     4917.114164\n",
       "organizar       4980.415728\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsm.neighbors(\"senado\", infoleg5, distfunc=vsm.euclidean).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploración de vecinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "senado        0.000000\n",
       "diputados     0.030828\n",
       "promulgada    0.032931\n",
       "reunidos      0.034499\n",
       "sancionan     0.056726\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsm.neighbors(\"senado\", infoleg20, distfunc=vsm.cosine).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "senado            0.000000\n",
       "reunidos       8939.967897\n",
       "diputados     10581.720843\n",
       "promulgada    11636.085811\n",
       "sancionan     12191.526525\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsm.neighbors(\"senado\", infoleg20, distfunc=vsm.euclidean).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Métodos de reponderación\n",
    "\n",
    "1. [Objetivos de reponderar](#Objetivos-de-reponderar)\n",
    "1. [Normalización de conteo](#Normalización-de-conteo)\n",
    "1. [Observado/Esperado](#Observado/Esperado)\n",
    "1. [Información mutua punto a punto](#Información-mutua-punto-a-punto)\n",
    "1. [TF-IDF](#TF-IDF)\n",
    "1. [Distribución de valores de acuerdo a esquemas de reponderación](#Distribución-de-valores-de-acuerdo-a-esquemas-de-reponderación)\n",
    "1. [Observaciones respecto a reponderación](#Observaciones-respecto-a-reponderación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Objetivos de reponderar\n",
    "\n",
    "- Amplificar lo importante, lo confiable o lo inusual. Minimizar lo mundano o lo común.\n",
    "- La idea de moverse del conteo plano es que las frecuencias son una aproximación pobre.\n",
    "- Sobre cada esquema de reponderación hay que preguntarse:\n",
    "    - ¿Cómo se compara con valores de conteo sin procesar?\n",
    "    - ¿Cómo se compara con la frecuencia de palabras?\n",
    "    - ¿Qué distribución general de valores devuelve?\n",
    "- Se buscan métodos que no requieran intervención extra (e.g. remoción de palabras vacías, selección de atributos, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalización de conteo\n",
    "\n",
    "- Es la forma más básica para hacer reponderación.\n",
    "- Se vió anteriormente como realizar [normalización mediante longitud L2](#Normalización-del-vector).\n",
    "- Se puede normalizar además, mediante la suma de sus valores, y obtener una distribución de probabilidad:\n",
    "\n",
    "$$\\left[ \\frac{u_{1}}{\\sum_{i=1}^{n}u_{i}}, \\frac{u_{2}}{\\sum_{i=1}^{n}u_{i}}, \\ldots, \\frac{u_{n}}{\\sum_{i=1}^{n}u_{i}} \\right]$$\n",
    "\n",
    "- Estas normalizaciones son **insensibles a cambios en la magnitud**, algo que puede ser perjudicial cuando tenemos muchos datos:\n",
    "    - Los vectores $[1, 10]$ y $[1000,10000]$ son muy distintos, pero la normalización lo oscurecerá."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observado/Esperado\n",
    "\n",
    "- El cálculo del valor esperado define una celda si las dos palabras fueran independientes. \n",
    "- Al utilizar el valor observado y dividirlo por el esperado, se amplifican aquellos valores cuya observación supera a la esperanza.\n",
    "\n",
    "\n",
    "$$\\textbf{rowsum}(X, i) = \\sum_{j=1}^{n}X_{ij}$$\n",
    "\n",
    "$$\\textbf{colsum}(X, j) = \\sum_{i=1}^{m}X_{ij}$$\n",
    "\n",
    "$$\\textbf{sum}(X) = \\sum_{i=1}^{m}\\sum_{j=1}^{n} X_{ij}$$\n",
    "\n",
    "$$\\textbf{expected}(X, i, j) = \n",
    "\\frac{\n",
    "  \\textbf{rowsum}(X, i) \\cdot \\textbf{colsum}(X, j)\n",
    "}{\n",
    "  \\textbf{sum}(X)\n",
    "}$$\n",
    "\n",
    "$$\\textbf{oe}(X, i, j) = \\frac{X_{ij}}{\\textbf{expected}(X, i, j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observado/Esperado\n",
    "\n",
    "<img src=\"img/observed-expected-1.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observado/Esperado\n",
    "\n",
    "<img src=\"img/observed-expected-2.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observado/Esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "senado       0.000000\n",
       "cámara       0.384637\n",
       "nación       0.576544\n",
       "reunidos     0.581250\n",
       "diputados    0.667229\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoleg5_oe = vsm.observed_over_expected(infoleg5)\n",
    "vsm.neighbors(\"senado\", infoleg5_oe).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "senado        0.000000\n",
       "reunidos      0.168378\n",
       "diputados     0.171603\n",
       "promulgada    0.185595\n",
       "etc.          0.225616\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoleg20_oe = vsm.observed_over_expected(infoleg20)\n",
    "vsm.neighbors(\"senado\", infoleg20_oe).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Información mutua punto a punto\n",
    "\n",
    "- La información mutua punto a punto (*PMI* en inglés), es observado/esperado en espacio logarítmico:\n",
    "\n",
    "$$\\textbf{pmi}(X, i, j) = \\log\\left(\\frac{X_{ij}}{\\textbf{expected}(X, i, j)}\\right)$$\n",
    "\n",
    "- Tiene un problema para co-ocurrencias nulas. Si definimos $\\log(0) = 0$ tenemos un problema:\n",
    "    - Observado > Esperado $\\Rightarrow$ PMI alta.\n",
    "    - Observado < Esperado $\\Rightarrow$ PMI baja.\n",
    "    - Conteo $0 \\Rightarrow$ En el medio de las anteriores.\n",
    "    \n",
    "- La forma de lidiar con esto: utilizar **PMI Positiva**:\n",
    "\n",
    "$$\\textbf{ppmi}(X, i, j) = \\max(0, \\textbf{pmi}(X, i, j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Información mutua punto a punto\n",
    "\n",
    "<img src=\"img/pmi.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Información mutua punto a punto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "senado        0.000000\n",
       "promulgada    0.414552\n",
       "setiembre     0.494921\n",
       "noviembre     0.506653\n",
       "octubre       0.514989\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoleg5_ppmi = vsm.pmi(infoleg5, positive=True)\n",
    "vsm.neighbors(\"senado\", infoleg5_ppmi).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "senado        0.000000\n",
       "sancionan     0.162767\n",
       "promulgada    0.197669\n",
       "diputados     0.202006\n",
       "sancionada    0.255992\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoleg20_ppmi = vsm.pmi(infoleg20, positive=True)\n",
    "vsm.neighbors(\"senado\", infoleg20_ppmi).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "- Uno de los esquemas de reponderación más conocidos: **Term Frequency–Inverse Document Frequency (TF-IDF)** (literalmente: _Frecuencia de Palabra–Frecuencia Inversa de Documento_). Muy utilizado por los buscadores web.\n",
    "- Está definido en base a las medidas de *TF* e *IDF*. Para una matriz de $m$ términos y $n$ documentos, se definen:\n",
    "\n",
    "$$\\textbf{TF}(X, i, j) = \\frac{X_{ij}}{\\textbf{colsum}(X, i, j)}$$\n",
    "\n",
    "$$\\textbf{IDF}(X, i, j) = \\log\\left(\\frac{n}{|\\{k : X_{ik} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\textbf{TF-IDF}(X, i, j) = \\textbf{TF}(X, i, j) \\cdot \\textbf{IDF}(X, i, j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "<img src=\"img/tfidf.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "- TF-IDF suele funcionar mejor con matrices ralas.\n",
    "- Está pensado más bien para una matriz de documentos, en lugar de una matriz de co-ocurrencia de palabras.\n",
    "- En particular, no funcionaría bien para nuestros datos de InfoLEG.\n",
    "    - Si modelamos una matriz de documentos (leyes, artículos, resoluciones, etc.) y palabras, TF-IDF tiene más sentido.\n",
    "\n",
    "__Importante__: Notar que [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer), de `scikit-learn` asume que la matriz de documentos utiliza los documentos como fila y las palabras como columna (serían los _atributos_ del modelo). Esto tiene más sentido para un trabajo de clasificación. El paquete `vsm` posee una implementación bastante sencilla de TF-IDF para documentos en forma de columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Equivalente a CountVectorizer + TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "vectorizer = TfidfVectorizer(\"filename\", max_features=5000)\n",
    "corpus = sorted(os.path.join(\"./data/infoleg_text\", fname) for fname in os.listdir(\"./data/infoleg_text/\"))\n",
    "vectorized_corpus = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distribución de valores de acuerdo a esquemas de reponderación\n",
    "\n",
    "<img src=\"img/weighting-scheme-distribution.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observaciones respecto a reponderación\n",
    "\n",
    "- Se pesa el valor de una celda $X_{ij}$ respecto al valor esperado que dan $X_{i*}$ y $X_{*j}$.\n",
    "- Algunos esquemas terinan favoreciendo eventos muy raros (ver PMI). Esto puede derivar en amplificación de ruido, algo bastante común cuando lidiamos con datos de lenguaje.\n",
    "- La magnitud puede ser importante, por lo que esquemas de normalización llana pueden oscurecer información valiosa.\n",
    "- TF-IDF castiga severamente las palabras que aparecen en muchos documentos, por lo que no funciona bien con un esquema denso como el de matrices de co-ocurrencias entre palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Información de subpalabras\n",
    "\n",
    "1. [Objetivos de usar subpalabras](#Objetivos-de-usar-subpalabras)\n",
    "1. [Representación de palabras mediante n-gramas de caracteres](#Representación-de-palabras-mediante-n-gramas-de-caracteres)\n",
    "1. [Evaluando palabras OOV](#Evaluando-palabras-OOV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Objetivos de usar subpalabras\n",
    "\n",
    "- Idea propuesta por Schütze (Schütze, 1993): utilizar información de subpalabras.\n",
    "- Busa mejorar representaciones reduciendo dispersión.\n",
    "- Incrementa las conexiones en un VSM.\n",
    "- Facilitar modelado de palabras OOV (fuera del vocabulario).\n",
    "- Reduce el impacto de los esquemas de tokenización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representación de palabras mediante n-gramas de caracteres\n",
    "\n",
    "- El equipo de [fastText](https://fasttext.cc/) (Bojanowski et al., 2017) propuso:\n",
    "    - A partir de un VSM de palabras, el vector de un n-grama de caracteres _x_ es la suma de todos los vectores de palabras que contengan a _x_\n",
    "    - Representar cada palabra _w_ como la suma de sus n-gramas de caracteres.\n",
    "    - Sumar la representación de _w_ si está disponible\n",
    "- La función `ngram_vsm` del módulo `vsm` puede lograr esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7506, 5001)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infoleg5_ngram = vsm.ngram_vsm(infoleg5, n=4)\n",
    "infoleg5_ngram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Tiene la misma dimensión de columnas que `infoleg5`, pero las filas fueron expandidas con todos los 4-gramas, incluyendo símbolos de límite `<w>` y `</w>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluando palabras OOV\n",
    "\n",
    "- Con el nuevo modelo, podemos representar nuevas palabras a partir de sus n-gramas.\n",
    "- Esto da pie a evaluar palabras fuera del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def character_level_rep(word, cf, n=4):\n",
    "    ngrams = vsm.get_character_ngrams(word, n)\n",
    "    ngrams = [n for n in ngrams if n in cf.index]    \n",
    "    reps = cf.loc[ngrams].values\n",
    "    return reps.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'subrepresentado' in infoleg5.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005210874154534251"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subrepresentado = character_level_rep('subrepresentado', infoleg5_ngram, 4)\n",
    "representado = character_level_rep('representado', infoleg5_ngram, 4)\n",
    "vsm.cosine(subrepresentado, representado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "- Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5, 135-146.\n",
    "- Firth, J. R. (1957). A synopsis of linguistic theory, 1930-1955. Studies in linguistic analysis.\n",
    "- Harris, Z. S. (1954). Distributional structure. Word, 10(2-3), 146-162.\n",
    "- Schütze, H. (1993). Word space. In Advances in neural information processing systems (pp. 895-902).\n",
    "- Turney, P. D., & Pantel, P. (2010). From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37, 141-188.\n",
    "- Wittgenstein, L. (1953). Philosophical investigations. (Translated by GE Anscombe) Oxford: Blackwell.\n",
    "- Zipf, G. K. (1949). Human behavior and the principle of least effort."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
