{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representación de palabras mediante espacios vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Créditos\n",
    "\n",
    "Estas clases y material están fuertemente inspirado en los cursos de Stanford [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/) y [CS224u: Natural Language Understanding](http://web.stanford.edu/class/cs224u/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "__author__ = \"Cristian Cardellino\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contenidos\n",
    "\n",
    "1. [Representación de palabras](#Representación-de-palabras)\n",
    "1. [Representación distribuída de palabras](#Representación-distribuída-de-palabras)\n",
    "1. Diseño de matrices\n",
    "    1. Matriz de co-ocurrencia de palabras\n",
    "    1. Matriz de palabra-vector\n",
    "    1. Otros tipos de matrices\n",
    "    1. Ventana y ajuste\n",
    "1. Comparación de vectores\n",
    "    1. Distancia euclídea\n",
    "    1. Normalización de longitud\n",
    "    1. Distancia (y similitud) coseno\n",
    "    1. Otras medidas\n",
    "    1. Propiedades de una distancia\n",
    "    1. Generalidades\n",
    "1. Métodos de reponderación\n",
    "    1. Objetivos de reponderar\n",
    "    1. Normalización\n",
    "    1. Observado/Esperado\n",
    "    1. Información mutua\n",
    "    1. TF-IDF\n",
    "    1. Generalidades\n",
    "1. Visualización\n",
    "    1. PCA\n",
    "    1. t-SNE\n",
    "1. Reducción de dimensionalidad\n",
    "    1. LSA\n",
    "    1. Autoencoders\n",
    "    1. word2vec\n",
    "    1. GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representación de palabras\n",
    "\n",
    "1. [Características de las palabras](#Características-de-las-palabras)\n",
    "1. [Representación mediante taxonomías](#Representación-mediante-taxonomías)\n",
    "1. [Representación discreta](#Representación-discreta)\n",
    "1. [Ejemplo de representación discreta](#Ejemplo-de-representación-discreta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Características de las palabras\n",
    "\n",
    "- Son la **unidad básica** en cualquier tarea de procesamiento de lenguaje natural (PLN).\n",
    "- Cualquier modelo de PLN requiere como entrada la **representación de una palabra**.\n",
    "- Gran parte del trabajo de PLN solía representar las palabras como **símbolos atómicos**.\n",
    "- Las nociones de **similitu y distancia** entre palabras son cruciales para tareas de PLN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representación mediante taxonomías\n",
    "\n",
    "- El **sentido** de una palabras es la idea que una palabra (o frase) representa.\n",
    "- Se puede representar mediante recursos del estilo de las **taxonomías**.\n",
    "- Ofrecen **información rica y estructurada**.\n",
    "    - WordNet asigna relaciones de **hiperonimia/hiponimia** (relación *es-un*), y **sinonimia**.\n",
    "    - FrameNet establece la **estructura** mediante un marco sintáctico y semántico de las palabras.\n",
    "- Son **difíciles y caros de obtener y actualizar** (requiren anotación humana)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog's hypernyms:\n",
      "Synset('canine.n.02')\n",
      "Synset('domestic_animal.n.01')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn  # Run nltk.download(\"wordnet\") first\n",
    "\n",
    "dog = wn.synset(\"dog.n.01\")\n",
    "\n",
    "print(\"Dog's hypernyms:\")\n",
    "for hypernym in dog.hypernyms():\n",
    "    print(hypernym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representación discreta\n",
    "\n",
    "- Para aplicar medidas de similitud y distancia (e.g. euclídea, coseno), utilizamos **vectores que representen palabras**.\n",
    "- Primera aproximación: **vectores *one-hot***.\n",
    "- Para cada palabra del vocabulario, tenemos un vector.\n",
    "- En términos de espacio vectorial, el vector tiene un **1** en una de sus dimensiones y __0s__ en todas las demás.\n",
    "- **Dimensionalidad muy alta** (el inglés tiene un estimado de 13 millones de palabras).\n",
    "- Los vectores son **ralos** (o esparsos), tienen muchos ceros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de representación discreta\n",
    "\n",
    "Dado un corpus:\n",
    "\n",
    "- \"El fallo fue decisivo\"\n",
    "- \"La corte rectificará el fallo\"\n",
    "\n",
    "Tenemos un vocabulario $V = \\{corte, decisivo, el, fallo, fue, la, rectificará\\}$.\n",
    "\n",
    "Codificamos un número $|V| = 7$ de vectores para cada palabra $w^{(i)} \\in \\mathbb{R}^{|V|}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de representación discreta\n",
    "\n",
    "$$\n",
    "    w^{corte} =\n",
    "    \\begin{bmatrix}\n",
    "       1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{decisivo} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{el} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{fallo} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "$$\n",
    "\n",
    "$$\n",
    "    w^{fue} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix}\n",
    "    w^{la} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{rectificará} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    \"El fallo fue decisivo\", \n",
    "    \"La corte rectificará el fallo\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [\n",
    "    [word.lower() for word in doc.split()] for doc in corpus\n",
    "]\n",
    "\n",
    "vocabulary = sorted(set([\n",
    "    word for doc in tokenized_corpus for word in doc\n",
    "]))\n",
    "\n",
    "one_hot_vectors = np.eye(len(vocabulary), dtype=np.int)\n",
    "\n",
    "for doc in tokenized_corpus:\n",
    "    print(np.array([\n",
    "        one_hot_vectors[vocabulary.index(word), :] for word in doc\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representación distribuída de palabras\n",
    "\n",
    "1. Objetivos de alto nivel\n",
    "1. Hipótesis sobre representación distribuída"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
