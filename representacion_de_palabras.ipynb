{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representación de palabras mediante espacios vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Créditos\n",
    "\n",
    "Estas clases y material están fuertemente inspirado en los cursos de Stanford [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/) y [CS224u: Natural Language Understanding](http://web.stanford.edu/class/cs224u/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "__author__ = \"Cristian Cardellino\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contenidos\n",
    "\n",
    "1. [Representación de palabras](#Representación-de-palabras)\n",
    "1. [Representación distribuída de palabras](#Representación-distribuída-de-palabras)\n",
    "1. [Diseño de matrices](#Diseño-de-matrices)\n",
    "1. [Comparación de vectores](#Comparación-de-vectores)\n",
    "1. [Métodos de reponderación](#Métodos-de-reponderación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import vsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representación de palabras\n",
    "\n",
    "1. [Características de las palabras](#Características-de-las-palabras)\n",
    "1. [Representación mediante taxonomías](#Representación-mediante-taxonomías)\n",
    "1. [Representación discreta](#Representación-discreta)\n",
    "1. [Ejemplo de representación discreta](#Ejemplo-de-representación-discreta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Características de las palabras\n",
    "\n",
    "- Son la **unidad básica** en cualquier tarea de procesamiento de lenguaje natural (PLN).\n",
    "- Cualquier modelo de PLN requiere como entrada la **representación de una palabra**.\n",
    "- Gran parte del trabajo de PLN solía representar las palabras como **símbolos atómicos**.\n",
    "- Las nociones de **similitu y distancia** entre palabras son cruciales para tareas de PLN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representación mediante taxonomías\n",
    "\n",
    "- El **sentido** de una palabras es la idea que una palabra (o frase) representa.\n",
    "- Se puede representar mediante recursos del estilo de las **taxonomías**.\n",
    "- Ofrecen **información rica y estructurada**.\n",
    "    - WordNet asigna relaciones de **hiperonimia/hiponimia** (relación *es-un*), y **sinonimia**.\n",
    "    - FrameNet establece la **estructura** mediante un marco sintáctico y semántico de las palabras.\n",
    "- Son **difíciles y caros de obtener y actualizar** (requiren anotación humana)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn  # Run nltk.download(\"wordnet\") first\n",
    "\n",
    "dog = wn.synset(\"dog.n.01\")\n",
    "\n",
    "print(\"Dog's hypernyms:\")\n",
    "for hypernym in dog.hypernyms():\n",
    "    print(hypernym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representación discreta\n",
    "\n",
    "- Para aplicar medidas de similitud y distancia (e.g. euclídea, coseno), utilizamos **vectores que representen palabras**.\n",
    "- Primera aproximación: **vectores *one-hot***.\n",
    "- Para cada palabra del vocabulario, tenemos un vector.\n",
    "- En términos de espacio vectorial, el vector tiene un **1** en una de sus dimensiones y __0s__ en todas las demás.\n",
    "- **Dimensionalidad muy alta** (el inglés tiene un estimado de 13 millones de palabras).\n",
    "- Los vectores son **ralos** (o esparsos), tienen muchos ceros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de representación discreta\n",
    "\n",
    "Dado un corpus:\n",
    "\n",
    "- \"El fallo fue decisivo\"\n",
    "- \"La corte rectificará el fallo\"\n",
    "\n",
    "Tenemos un vocabulario $V = \\{corte, decisivo, el, fallo, fue, la, rectificará\\}$.\n",
    "\n",
    "Codificamos un número $|V| = 7$ de vectores para cada palabra $w^{(i)} \\in \\mathbb{R}^{|V|}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de representación discreta\n",
    "\n",
    "$$\n",
    "    w^{corte} =\n",
    "    \\begin{bmatrix}\n",
    "       1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{decisivo} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{el} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{fallo} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "$$\n",
    "\n",
    "$$\n",
    "    w^{fue} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix}\n",
    "    w^{la} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{rectificará} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"El fallo fue decisivo\", \n",
    "    \"La corte rectificará el fallo\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [\n",
    "    [word.lower() for word in doc.split()] for doc in corpus\n",
    "]\n",
    "\n",
    "vocabulary = sorted(set([\n",
    "    word for doc in tokenized_corpus for word in doc\n",
    "]))\n",
    "\n",
    "one_hot_vectors = np.eye(len(vocabulary), dtype=np.int)\n",
    "\n",
    "for doc in tokenized_corpus:\n",
    "    print(np.array([\n",
    "        one_hot_vectors[vocabulary.index(word), :] for word in doc\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representación distribuída de palabras\n",
    "\n",
    "1. [Concepto](#Concepto)\n",
    "1. [Motivación](#Motivación)\n",
    "1. [Hipótesis para guiarse](#Hipótesis-para-guiarse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Concepto\n",
    "\n",
    "- Idea: Representar una palabra **mediante sus vecinos**.\n",
    "    - Es una de las **ideas más importantes** en PLN moderno.\n",
    "\n",
    "#### Ejemplo: \n",
    "\n",
    "- \"_El gobierno rescató a los_ **bancos** _ante la crisis de crédito_\".\n",
    "- \"_La municipalidad reparará los_ **bancos** _de la plaza_\".\n",
    "\n",
    "En estos ejemplos, las palabras que están en _cursiva_ representarán las palabras en **negrita**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Motivación\n",
    "\n",
    "- La representación distribuída nos da pie a pensar como los vectores **codifican el significado de las unidades linguísticas** (palabras, oraciones, documentos, etc.).\n",
    "- Son la base de los **modelos de espacio vectorial**.\n",
    "- Piezas fundamentales en modelos para **Procesamiento y Comprensión de Lenguaje Natural**.\n",
    "- Estas representaciones pueden ser usadas, entre otros, para:\n",
    "    - Entender y modelar fenómenos sociales y lingüísticos.\n",
    "    - Entrada para modelos de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hipótesis para guiarse\n",
    "\n",
    "- \"You shall know a word by the company it keeps\" (Firth, 1957).\n",
    "- \"The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously\" (Firth, 1957).\n",
    "- \"The meaning of a word is its use in the language\" (Wittgenstein, 1957).\n",
    "- \"Distributional statements can cover all of the material of a language without requiring support from other types of information\" (Harris, 1954)\n",
    "- \"If units of text have similar vectors in a text frequency matrix, then they tend to have similar meanings.\" (Turney & Pantel, 2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diseño de matrices\n",
    "\n",
    "1. [Objetivos del diseño de matrices](#Objetivos-del-diseño-de-matrices)\n",
    "1. [Matriz de co-ocurrencia de palabras](#Matriz-de-co-ocurrencia-de-palabras)\n",
    "1. [Matriz de documentos](#Matriz-de-documentos)\n",
    "1. [Ventana y ponderación](#Ventana-y-ponderación)\n",
    "1. [Más diseños de matrices](#Más-diseños-de-matrices)\n",
    "1. [Consideraciones](#Consideraciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Objetivos del diseño de matrices\n",
    "\n",
    "- Buscamos representar palabras de manera matemática.\n",
    "    - Idea: Mediante una matriz (vista como un conjunto de vectores)\n",
    "- Hay que definir la construcción de una matriz.\n",
    "    - Muchas opciones de diseño.\n",
    "    - Distinto impacto tendrán las decisiones tomadas respecto al modelado del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matriz de co-ocurrencia de palabras\n",
    "\n",
    "- Cada palabra se representa por las palabras a su alrededor (dada una ventana).\n",
    "- La matriz es simétrica (independiente de la posición de las palabras).\n",
    "    - Esto se conoce como \"bolsa de palabras\" (o _bag of words_)\n",
    "- Dado un vocabulario $V$:\n",
    "    - Matriz de co-ocurrencias $X \\in \\mathbb{R}^{|V|\\times|V|}$\n",
    "    - $X_{ij}$ representa la co-ocurrencia entre la palabras $w^{(i)}$ y $w^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matriz de co-ocurrencia de palabras\n",
    "\n",
    "<img src=\"img/word-word-matrix.png\" width=\"90%\"/>\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matriz de documentos\n",
    "\n",
    "- Cada documento se representa por las palabras que lo conforman.\n",
    "- Se usa un modelo de bolsa de palabras: no importa la posición.\n",
    "- Dadas $N$ palabras en un corpus de $M$ documentos:\n",
    "    - Matriz de documentos $X \\in \\mathbb{R}^{N \\times M}$\n",
    "    - $X_{ij}$ representa la ocurrencia de la palabra $w^{(i)}$ en el documento $d^{(j)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matriz de documentos\n",
    "\n",
    "<img src=\"img/term-document-matrix.png\" width=\"90%\"/>\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ventana y ponderación\n",
    "\n",
    "- La ventana define la **cantidad de palabras en el contexto** a considerar.\n",
    "- La ponderación (*scaling*) le da un **peso a la palabra** en la suma total.\n",
    "    - Igual peso para todas las palabras, i.e. bolsa de palabras.\n",
    "    - Peso distinto de acuerdo a la distancia, e.g. decaimiento fraccional.\n",
    "    - Peso binario, i.e. la palabra está o no.\n",
    "\n",
    "<img src=\"img/window-scaling.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ventana y ponderación\n",
    "\n",
    "- Ventanas más grandes y con peso igual (_flat_) capturan **información semántica**.\n",
    "    - Las matrices de documentos dan idea de la temática del documento.\n",
    "- Ventanas más cortas y con pesos capturan **información sintáctica** (colocacional).\n",
    "    - Las matrices de co-ocurrencias de palabras son de afinidad de palabras.\n",
    "- El límite puede definirse más allá de una ventana fija.\n",
    "    - Utilizar oraciones, párrafos o documentos tendrá consecuencias distintas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "infoleg5 = pd.read_csv(\"./data/infoleg_5window_scaled.csv.gz\", index_col=0)\n",
    "\n",
    "infoleg20 = pd.read_csv(\"./data/infoleg_20window_flat.csv.gz\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Más diseños de matrices\n",
    "\n",
    "- palabra $\\times$ relación de dependencia\n",
    "- palabra $\\times$ contexto sintáctico\n",
    "- adjetivo $\\times$ sustantivo modificado\n",
    "- palabra $\\times$ consulta de búsqueda\n",
    "- persona $\\times$ producto\n",
    "- palabra $\\times$ persona\n",
    "- verbo $\\times$ sujeto $\\times$ objeto\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Consideraciones\n",
    "\n",
    "- Las matrices **incrementan con el vocabulario**.\n",
    "    - Definir límites en vocabulario o bien lidiar con modelos muy grandes.\n",
    "    - Por la ley de Zipf (Zipf, 1949), la lista de palabras con escasa aparición se vuelve muy grande.\n",
    "- La elección de la ventana afecta en la obtención de **matrices más o menos dispersas**.\n",
    "- Representar distintas unidades lingüísticas deriva en **más dispersión** (ver el ejemplo de word-word vs term-document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparación de vectores \n",
    "\n",
    "1. [Objetivos de comparar vectores](#Objetivos-de-comparar-vectores)\n",
    "1. [Ejemplo de juguete](#Ejemplo-de-juguete)\n",
    "1. [Distancia euclídea](#Distancia-euclídea)\n",
    "1. [Normalización del vector](#Normalización-del-vector)\n",
    "1. [Distancia coseno](#Distancia-coseno)\n",
    "1. [Medidas basadas en coincidencias](#Medidas-basadas-en-coincidencias)\n",
    "1. [Propiedades de una distancia](#Propiedades-de-una-distancia)\n",
    "1. [Otras medidas](#Otras-medidas)\n",
    "1. [Exploración de vecinos](#Exploración-de-vecinos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Objetivos de comparar vectores\n",
    "\n",
    "- Está en el núcleo de nuestro análisis.\n",
    "- Generalmente buscamos **medir distancias** entre vectores.\n",
    "    - La idea es que palabras relacionadas estén cercanas en el espacio vectorial construído.\n",
    "- El módulo [scipy.spatial.distance](http://docs.scipy.org/doc/scipy-0.14.0/reference/spatial.distance.html) ofrece una gran cantidad de métricas para comparar vectores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de juguete\n",
    "\n",
    "<img src=\"img/vector-toy-example.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de juguete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ABC = pd.DataFrame([\n",
    "    [ 2.0,  4.0], \n",
    "    [10.0, 15.0], \n",
    "    [14.0, 10.0]],\n",
    "    index=['A', 'B', 'C'],\n",
    "    columns=['dx', 'dy'])\n",
    "\n",
    "ABC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de juguete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_ABC(df):\n",
    "    ax = df.plot.scatter(x='dx', y='dy', marker='.', legend=False)\n",
    "    m = df.values.max(axis=None)\n",
    "    ax.set_xlim([0, m*1.2])\n",
    "    ax.set_ylim([0, m*1.2])\n",
    "    for label, row in df.iterrows():\n",
    "        ax.text(row['dx'], row['dy'], label)\n",
    "\n",
    "plot_ABC(ABC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distancia euclídea\n",
    "\n",
    "- Es la **distancia** más básica e intuitiva entre dos vectores.\n",
    "- La distancia euclídea entre dos vectores $u$ y $v$ de dimensión $n$ se define como:\n",
    "\n",
    "$$\\textbf{euclidean}(u, v) = \\sqrt{\\sum_{i=1}^{n}|u_{i} - v_{i}|^{2}}$$\n",
    "\n",
    "- En dos dimensiones, se corresponde con la longitud de la línea más directa entre dos puntos.\n",
    "\n",
    "<img src=\"img/euclidean-distance.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distancia euclídea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def abc_comparisons(df, distfunc):\n",
    "    for a, b in (('A', 'B'), ('B', 'C')):\n",
    "        dist = distfunc(df.loc[a], df.loc[b])\n",
    "        print(f\"{distfunc.__name__}({a}, {b}) = {dist:7.02f}\")\n",
    "\n",
    "abc_comparisons(ABC, vsm.euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Suponiendo que estos vectores fueran de palabras, hay algunos aspectos extraños:\n",
    "    - Notar que las distribuciones de B y C son opuestas (e.g. \"bueno\" vs. \"malo\").\n",
    "    - A y B están más alineadas en cambio, salvo por las frecuencias (e.g. \"bueno\" vs. \"excelente\").\n",
    "- La distancia euclídea está **sesgada por tamaño**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalización del vector\n",
    "\n",
    "- Buscamos quitar el sesgo. Para eso **normalizamos el vector de acuerdo a su longitud**.\n",
    "- Definimos la **norma L2** de un vector:\n",
    "\n",
    "$$\\|u\\|_{2} = \\sqrt{\\sum_{i=1}^{n} u_{i}^{2}}$$\n",
    "\n",
    "- Luego, la normalización de $u$ se consigue dividiendo por su longitud (que es escalar):\n",
    "\n",
    "$$\\left[\\frac{u_{1}}{\\|u\\|_{2}}, \\frac{u_{2}}{\\|u\\|_{2}}, \\ldots, \\frac{u_{n}}{\\|u\\|_{2}}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalización del vector\n",
    "\n",
    "<img src=\"img/length-normalization.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalización del vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ABC_normed = ABC.apply(vsm.length_norm, axis=1)\n",
    "plot_ABC(ABC_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "abc_comparisons(ABC_normed, vsm.euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distancia coseno\n",
    "\n",
    "- La **distancia coseno toma toda la longitud en cuenta**.\n",
    "- La distancia coseno entre dos vectores $u$ y $v$ de dimensión $n$ se define como:\n",
    "\n",
    "$$\\textbf{cosine}(u, v) = 1 - \\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|_{2} \\cdot \\|v\\|_{2}}$$\n",
    "\n",
    "- La similitud coseno (el término que se está restando), **mide el ángulo entre dos vectores**.\n",
    "- Es **equivalente** a tomar la distancia euclídea de los vectores normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "abc_comparisons(ABC, vsm.cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distancia coseno\n",
    "\n",
    "<img src=\"img/cosine-distance-1.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distancia coseno\n",
    "\n",
    "<img src=\"img/cosine-distance-2.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Medidas basadas en coincidencias\n",
    "\n",
    "- El método básico basado en coincidencia crea un vector que mide la cantidad de coincidencias entre dos vectores:\n",
    "\n",
    "$$\\textbf{matching}(u, v) = \\sum_{i=1}^{n} \\min(u_{i}, v_{i})$$\n",
    "\n",
    "- El [__coeficiente de Jaccard__](https://en.wikipedia.org/wiki/Jaccard_index) es una manera de normalizar el método de coincidencias.\n",
    "\n",
    "$$\\textbf{jaccard}(u, v) = 1 - \\frac{\\textbf{matching}(u, v)}{\\sum_{i=1}^{n} \\max(u_{i}, v_{i})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "abc_comparisons(ABC, vsm.jaccard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Propiedades de una distancia\n",
    "\n",
    "Para que una medida califique como distancia, debe cumplir algunos requisitos.\n",
    "\n",
    "- Simétrica: $d(u, v) = d(v, u)$.\n",
    "- Asignar 0 a vectores idénticos: $d(v, v) = 0$\n",
    "- Satisfacer la **desigualdad triangular**: $d(u, w) \\leq d(u,v) + d(v, w)$\n",
    "    - La distancia coseno, definida arriba, no satisface esta última (hay otras maneras de definirla)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Otras medidas\n",
    "\n",
    "Muchas otras medidas existen con ligeras variaciones. La mayoría se encuentran implementadas en [scipy](https://www.scipy.org/).\n",
    "\n",
    "- [Coeficiente de Dice](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)\n",
    "- [Coeficiente de superposición](https://en.wikipedia.org/wiki/Overlap_coefficient)\n",
    "- [KL Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "    - Symmetric KL\n",
    "    - KL-divergence with Skew\n",
    "    - [Jensen-Shannon](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\n",
    "\n",
    "De estas, sólo Jensen-Sahnnon es una métrica de distancia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploración de vecinos\n",
    "\n",
    "- La función `vsm.neighbors` es útil para investigación.\n",
    "- Para una palabra dada `w`, hace un ranking de todas las palabras en el vocabulario de acuerdo a su distancia con `w`, medida por `distfunc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vsm.neighbors(\"senado\", infoleg5, distfunc=vsm.cosine).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vsm.neighbors(\"senado\", infoleg5, distfunc=vsm.euclidean).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploración de vecinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vsm.neighbors(\"senado\", infoleg20, distfunc=vsm.cosine).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vsm.neighbors(\"senado\", infoleg20, distfunc=vsm.euclidean).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Métodos de reponderación\n",
    "\n",
    "1. [Objetivos de reponderar](#Objetivos-de-reponderar)\n",
    "1. [Normalización de conteo](#Normalización-de-conteo)\n",
    "1. [Observado/Esperado](#Observado/Esperado)\n",
    "1. [Información mutua punto a punto](#Información-mutua-punto-a-punto)\n",
    "1. [TF-IDF](#TF-IDF)\n",
    "1. [Distribución de valores de acuerdo a esquemas de reponderación](#Distribución-de-valores-de-acuerdo-a-esquemas-de-reponderación)\n",
    "1. [Observaciones respecto a reponderación](#Observaciones-respecto-a-reponderación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Objetivos de reponderar\n",
    "\n",
    "- Amplificar lo importante, lo confiable o lo inusual. Minimizar lo mundano o lo común.\n",
    "- La idea de moverse del conteo plano es que las frecuencias son una aproximación pobre.\n",
    "- Sobre cada esquema de reponderación hay que preguntarse:\n",
    "    - ¿Cómo se compara con valores de conteo sin procesar?\n",
    "    - ¿Cómo se compara con la frecuencia de palabras?\n",
    "    - ¿Qué distribución general de valores devuelve?\n",
    "- Se buscan métodos que no requieran intervención extra (e.g. remoción de palabras vacías, selección de atributos, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalización de conteo\n",
    "\n",
    "- Es la forma más básica para hacer reponderación.\n",
    "- Se vió anteriormente como realizar [normalización mediante longitud L2](#Normalización-del-vector).\n",
    "- Se puede normalizar además, mediante la suma de sus valores, y obtener una distribución de probabilidad:\n",
    "\n",
    "$$\\left[ \\frac{u_{1}}{\\sum_{i=1}^{n}u_{i}}, \\frac{u_{2}}{\\sum_{i=1}^{n}u_{i}}, \\ldots, \\frac{u_{n}}{\\sum_{i=1}^{n}u_{i}} \\right]$$\n",
    "\n",
    "- Estas normalizaciones son **insensibles a cambios en la magnitud**, algo que puede ser perjudicial cuando tenemos muchos datos:\n",
    "    - Los vectores $[1, 10]$ y $[1000,10000]$ son muy distintos, pero la normalización lo oscurecerá."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observado/Esperado\n",
    "\n",
    "- El cálculo del valor esperado define una celda si las dos palabras fueran independientes. \n",
    "- Al utilizar el valor observado y dividirlo por el esperado, se amplifican aquellos valores cuya observación supera a la esperanza.\n",
    "\n",
    "\n",
    "$$\\textbf{rowsum}(X, i) = \\sum_{j=1}^{n}X_{ij}$$\n",
    "\n",
    "$$\\textbf{colsum}(X, j) = \\sum_{i=1}^{m}X_{ij}$$\n",
    "\n",
    "$$\\textbf{sum}(X) = \\sum_{i=1}^{m}\\sum_{j=1}^{n} X_{ij}$$\n",
    "\n",
    "$$\\textbf{expected}(X, i, j) = \n",
    "\\frac{\n",
    "  \\textbf{rowsum}(X, i) \\cdot \\textbf{colsum}(X, j)\n",
    "}{\n",
    "  \\textbf{sum}(X)\n",
    "}$$\n",
    "\n",
    "$$\\textbf{oe}(X, i, j) = \\frac{X_{ij}}{\\textbf{expected}(X, i, j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observado/Esperado\n",
    "\n",
    "<img src=\"img/observed-expected-1.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observado/Esperado\n",
    "\n",
    "<img src=\"img/observed-expected-2.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observado/Esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "infoleg5_oe = vsm.observed_over_expected(infoleg5)\n",
    "vsm.neighbors(\"senado\", infoleg5_oe).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "infoleg20_oe = vsm.observed_over_expected(infoleg20)\n",
    "vsm.neighbors(\"senado\", infoleg20_oe).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Información mutua punto a punto\n",
    "\n",
    "- La información mutua punto a punto (*PMI* en inglés), es observado/esperado en espacio logarítmico:\n",
    "\n",
    "$$\\textbf{pmi}(X, i, j) = \\log\\left(\\frac{X_{ij}}{\\textbf{expected}(X, i, j)}\\right)$$\n",
    "\n",
    "- Tiene un problema para co-ocurrencias nulas. Si definimos $\\log(0) = 0$ tenemos un problema:\n",
    "    - Observado > Esperado $\\Rightarrow$ PMI alta.\n",
    "    - Observado < Esperado $\\Rightarrow$ PMI baja.\n",
    "    - Conteo $0 \\Rightarrow$ En el medio de las anteriores.\n",
    "    \n",
    "- La forma de lidiar con esto: utilizar **PMI Positiva**:\n",
    "\n",
    "$$\\textbf{ppmi}(X, i, j) = \\max(0, \\textbf{pmi}(X, i, j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Información mutua punto a punto\n",
    "\n",
    "<img src=\"img/pmi.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Información mutua punto a punto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "infoleg5_ppmi = vsm.pmi(infoleg5, positive=True)\n",
    "vsm.neighbors(\"senado\", infoleg5_ppmi).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "infoleg20_ppmi = vsm.pmi(infoleg20, positive=True)\n",
    "vsm.neighbors(\"senado\", infoleg20_ppmi).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "- Uno de los esquemas de reponderación más conocidos: **Term Frequency–Inverse Document Frequency (TF-IDF)** (literalmente: _Frecuencia de Palabra–Frecuencia Inversa de Documento_). Muy utilizado por los buscadores web.\n",
    "- Está definido en base a las medidas de *TF* e *IDF*. Para una matriz de $m$ términos y $n$ documentos, se definen:\n",
    "\n",
    "$$\\textbf{TF}(X, i, j) = \\frac{X_{ij}}{\\textbf{colsum}(X, i, j)}$$\n",
    "\n",
    "$$\\textbf{IDF}(X, i, j) = \\log\\left(\\frac{n}{|\\{k : X_{ik} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\textbf{TF-IDF}(X, i, j) = \\textbf{TF}(X, i, j) \\cdot \\textbf{IDF}(X, i, j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "<img src=\"img/tfidf.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "- TF-IDF suele funcionar mejor con matrices ralas.\n",
    "- Está pensado más bien para una matriz de documentos, en lugar de una matriz de co-ocurrencia de palabras.\n",
    "- En particular, no funcionaría bien para nuestros datos de InfoLEG.\n",
    "    - Si modelamos una matriz de documentos (leyes, artículos, resoluciones, etc.) y palabras, TF-IDF tiene más sentido.\n",
    "\n",
    "__Importante__: Notar que [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer), de `scikit-learn` asume que la matriz de documentos utiliza los documentos como fila y las palabras como columna (serían los _atributos_ del modelo). Esto tiene más sentido para un trabajo de clasificación. El paquete `vsm` posee una implementación bastante sencilla de TF-IDF para documentos en forma de columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Equivalente a CountVectorizer + TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "vectorizer = TfidfVectorizer(\"filename\", max_features=5000)\n",
    "corpus = sorted(os.path.join(\"./data/infoleg_text\", fname) for fname in os.listdir(\"./data/infoleg_text/\"))\n",
    "vectorized_corpus = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distribución de valores de acuerdo a esquemas de reponderación\n",
    "\n",
    "<img src=\"img/weighting-scheme-distribution.png\" width=\"75%\" />\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observaciones respecto a reponderación\n",
    "\n",
    "- Se pesa el valor de una celda $X_{ij}$ respecto al valor esperado que dan $X_{i*}$ y $X_{*j}$.\n",
    "- Algunos esquemas terinan favoreciendo eventos muy raros (ver PMI). Esto puede derivar en amplificación de ruido, algo bastante común cuando lidiamos con datos de lenguaje.\n",
    "- La magnitud puede ser importante, por lo que esquemas de normalización llana pueden oscurecer información valiosa.\n",
    "- TF-IDF castiga severamente las palabras que aparecen en muchos documentos, por lo que no funciona bien con un esquema denso como el de matrices de co-ocurrencias entre palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "- Firth, J. R. (1957). A synopsis of linguistic theory, 1930-1955. Studies in linguistic analysis.\n",
    "- Harris, Z. S. (1954). Distributional structure. Word, 10(2-3), 146-162.\n",
    "- Turney, P. D., & Pantel, P. (2010). From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37, 141-188.\n",
    "- Wittgenstein, L. (1953). Philosophical investigations. (Translated by GE Anscombe) Oxford: Blackwell.\n",
    "- Zipf, G. K. (1949). Human behavior and the principle of least effort."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
