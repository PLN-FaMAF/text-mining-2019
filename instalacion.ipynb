{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Instalación y configuración del entorno\n",
    "\n",
    "Este notebook contiene las instrucciones para instalar y configurar todas las herramientas necesarias para el curso de [Minería de Datos para Texto - FAMAF - 2019](https://sites.google.com/view/text-mining-2019).\n",
    "\n",
    "Las herramientas y notebooks de este repositorio están basados en el trabajo de Christopher Potts y Bill MacCartney para el curso de [Stanford CS224u - Natural Language Understanding](http://web.stanford.edu/class/cs224u/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "__author__ = \"Cristian Cardellino\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contenidos\n",
    "\n",
    "1. [Entorno de trabajo](#Entorno-de-trabajo)\n",
    "    1. [Descargar datos de NLTK](#Descargar-datos-de-NLTK)\n",
    "2. [Repositorio oficial](#Repositorio-oficial)\n",
    "    1. [Creación del entorno](#Creación-del-entorno)\n",
    "    2. [Activación del entorno](#Activación-del-entorno)\n",
    "    3. [Descargar datos de NLTK](##Descargar-datos-de-NLTK)\n",
    "3. [Paquetes extra](#Paquetes-extra)\n",
    "    1. [SpaCy](#SpaCy)\n",
    "        1. [Descargar modelos de SpaCy](#Descargar-modelos-de-SpaCy)\n",
    "        2. [¿Cómo utilizar SpaCy?](#¿Cómo-utilizar-SpaCy?)\n",
    "    2. [gensim](#gensim)\n",
    "        1. [¿Cómo utilizar gensim?](#¿Cómo-utilizar-gensim?)\n",
    "        2. [Respecto al uso de gensim](#Respecto-al-uso-de-gensim)\n",
    "4. [Algunos recursos extra](#Algunos-recursos-extra)\n",
    "    1. [Tutorial de Jupyter](#Tutorial-de-Jupyter)\n",
    "    2. [Tutorial de NumPy](#Tutorial-de-NumPy)\n",
    "    3. [Matemática](#Matemática)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entorno de trabajo\n",
    "\n",
    "Para este curso se recomienda el uso de la [distribución de Python Anaconda](https://www.anaconda.com/distribution/), que contiene la mayoría de los paquetes necesarios para trabajar: `IPython`, `Jupyter`, `Numpy`, `Scipy`, `matplotlib`, `scikit-learn`, `NLTK`, etc.\n",
    "\n",
    "Los usuarios más avanzados pueden utilizar su manejador de entornos favorito (e.g. `virtualenv`, `virtualenvwrapper`, `pipenv`, etc.), sin embargo, para la mayoría se recomienda la instalación única de Anaconda que facilita el proceso.\n",
    "\n",
    "El código y los notebooks de este curso están pensados para utilizar **Python 3.7**, si bien el código puede llegar a ser compatible con **Python 2.7**, no es prioridad mantener dicha compatibilidad.\n",
    "\n",
    "La instalación de Anaconda es bastante sencilla, sólo es necesario [descargar el paquete correspondiente](https://www.anaconda.com/distribution/#download-section) a su sistema operativo e instalarlo. Para una descripción detallada del procedimiento pueden revisar la [documentación oficial de instalación](https://docs.anaconda.com/anaconda/install/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Creación del entorno\n",
    "\n",
    "Una vez instalado Anaconda, se recomienda crear un entorno virtual para la materia, de manera de instalar allí todos los paquetes necesarios.\n",
    "\n",
    "    $ conda create -n text-mining python=3.7 anaconda\n",
    "\n",
    "Este comando crea un entorno llamado `text-mining` e instala todos los paquetes básicos de Anaconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activación del entorno\n",
    "\n",
    "Para activar el entorno, se utiliza el siguiente comando:\n",
    "\n",
    "    $ conda activate text-mining\n",
    "\n",
    "Una vez activado el entorno debería mostrarse el nombre en la línea de comandos de la consola, generalmente entre paréntesis. Para desactivar el entorno bien pueden salir de la sesión/terminal, o bien utilizar el siguiente comando:\n",
    "\n",
    "    (text-mining) $ conda deactivate text-mining\n",
    "\n",
    "Notar que a lo largo de estos notebooks, cuando un comando aparezca con `(text-mining)` esto quiere decir que debe ser corrido dentro del entorno creado para este curso, por lo que deben activarlo como se mostró anteriormente.\n",
    "\n",
    "Para más detalles sobre como manejar entornos en anaconda pueden referirse a la [documentación oficial sobre manejo de entornos](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Descargar datos de NLTK\n",
    "\n",
    "La [librería NLTK](http://www.nltk.org/) viene de la sigla \"Natural Language Toolkit\". Es una librería para hacer procesamiento de lenguaje natural.\n",
    "\n",
    "Si bien yo utilizo (y recomiendo) la librería [SpaCy](https://spacy.io), que está más actualizada y mejor pensada para un entorno de PLN industrial, la verdad es que NLTK todavía sigue siendo un buen recurso y vale la pena tenerlo (particularmente, tiene acceso libre a muchos corpus de distinta índole).\n",
    "\n",
    "En caso de querer descargar alguno de los conjuntos de datos (corpus) disponibles en NLTK, es tan sencillo como ejecutar lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Y elegir el corpus a descargar. En su defecto, utilizar el nombre del corpus a descargar:  `nltk.download(\"wordnet\")`.\n",
    "\n",
    "Este se descargará en `$NLTK_DATA`, que por defecto apunta a `$HOME/nltk_data`. Si esto no tiene mucho sentido (i.e. no provienen o están familiarizados con un entorno Linux, déjenlo como está)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Repositorio oficial\n",
    "\n",
    "El repositorio oficial del curso se encuentra públicamente disponible en GitHub:\n",
    "\n",
    "https://github.com/PLN-FaMAF/text-mining-2019\n",
    "\n",
    "Se recomienda aprender lo [básico de control de versiones](https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/) con `git`, puesto que para mantenerse actualizado con el material del curso se utilizará esta herramienta.\n",
    "\n",
    "Para clonar el repositorio:\n",
    "\n",
    "    $ git clone https://github.com/PLN-FaMAF/text-mining-2019.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Paquetes extra\n",
    "\n",
    "No todos los paquetes necesarios para este curso están disponibles en Anaconda. En particular, tres librerías necesarias deben ser instaladas aparte: `gensim`, `spacy` y `tensorflow`. Estos paquetes son recomendables de instalar a través de `pip` en lugar de `conda`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SpaCy\n",
    "\n",
    "[SpaCy](https://spacy.io/) es una herramienta completa para realizar tareas de Procesamiento de Lenguaje Natural a nivel industrial. Si bien Anaconda viene con `NLTK`, la realidad es que este último a quedado un poco desactualizado en algunos aspectos. SpaCy provee nuevos modelos y utilidades a la hora de hacer PLN (y un pipeline mucho más sencillo). La instalación de SpaCy se realiza con lo siguiente:\n",
    "\n",
    "    (text-mining) $ pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Descargar modelos de SpaCy\n",
    "\n",
    "Spacy ofrece [varios modelos para distintos idiomas](https://spacy.io/usage/models#languages). Estos tienen que ser descargados. Para ello se realiza el siguiente comando que se encargará de descargar los modelos:\n",
    "\n",
    "    (text-mining) $ python -m spacy download es\n",
    "\n",
    "El comando anterior descarga los modelos para el español. De acuerdo al idioma elegido se deberá cambiar el valor de `es` (e.g. `en` para inglés, `de` para alemán, etc.). \n",
    "\n",
    "A su vez, un [mismo idioma puede tener varios tipos de modelos](https://spacy.io/models/es). Los tipos generalmente varían en tamaño, pero a veces tienen atributos extras. De acuerdo al idioma, el modelo que se descarga por defecto puede ser de determinado tipo. En el caso del español, el modelo por defecto es el \"pequeño\" (sólo tiene dos modelos: pequeño y mediano). Para descargar el modelo \"mediano\", deberíamos haber utilizado `es_core_news_md` en lugar de simplemente `es`. \n",
    "\n",
    "Es recomendable leer la [documentación oficial (en inglés)](https://spacy.io/usage/models) para saber más acerca de los modelos existentes y como obtenerlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ¿Cómo utilizar SpaCy?\n",
    "\n",
    "Se recomienda leer el [tutorial básico de SpaCy](https://spacy.io/usage/spacy-101) sobre como arrancar con SpaCy. En todo caso, suponiendo que ya tengan los modelos descargados, al ejecutar la siguiente celda podrán hacer un análisis básico (aunque bastante completo) sobre la oración propuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es\")\n",
    "doc = nlp(\"La Corte Suprema de Justicia declaró inconstitucional la medida \" +\n",
    "          \"tomada por Decreto 13.456 del Poder Ejecutivo Nacional.\")\n",
    "\n",
    "print(\"Texto anotado\\n=============\\n\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_, token.head)\n",
    "\n",
    "\n",
    "print(\"\\n\\nEntidades encontradas\\n=====================\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### gensim\n",
    "\n",
    "La librería [gensim](https://radimrehurek.com/gensim/index.html) provee herramientas muy útiles para hacer tareas de PLN no supervisado. En este sentido, aporta una serie de herramientas para hacer \"análisis de temas\" (más conocido en inglés como _topic modeling_). En particular provee implementaciones muy eficientes en Python de algoritmos como [_LSA_](https://radimrehurek.com/gensim/models/lsimodel.html), [_LDA_](https://radimrehurek.com/gensim/models/ldamodel.html), [_TF-IDF_](https://radimrehurek.com/gensim/models/tfidfmodel.html), [_word2vec_](https://radimrehurek.com/gensim/models/word2vec.html) y [_fastText_](https://radimrehurek.com/gensim/models/fasttext.html).\n",
    "\n",
    "La instalación de `gensim`, al igual que SpaCy, se realiza mediante `pip`:\n",
    "\n",
    "    (text-mining) $ pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ¿Cómo utilizar gensim?\n",
    "\n",
    "Nuevamente, es recomendado introducirse a la herramienta [mediante el tutorial](https://radimrehurek.com/gensim/tutorial.html). Es una herramienta muy completa y tiene [varios modelos implementados](https://radimrehurek.com/gensim/apiref.html).\n",
    "\n",
    "Para realizar una pequeña prueba de concepto, pueden ejecutar la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "documents = [\n",
    "    \"La Corte Suprema de Justicia declaró inconstitucional la medida\",\n",
    "    \"El Máximo Tribunal de la Nación denegó la apelación\",\n",
    "    \"La corte de casación avaló la apelación\",\n",
    "    \"El Poder Ejecutivo Nacional apelará a la decisión del Tribunal\",\n",
    "    \"El recurso de casación fue dado de baja por falta de mérito\"\n",
    "]\n",
    "\n",
    "tokenized_documents = [\n",
    "    [word for word in document.lower().split()] for document in documents\n",
    "]\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_documents)\n",
    "corpus = [dictionary.doc2bow(tdoc) for tdoc in tokenized_documents]\n",
    "corpus_features = set(word for tdoc in tokenized_documents for word in tdoc)\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(corpus_features))\n",
    "\n",
    "similarities = index[tfidf[corpus[0]]]\n",
    "print(sorted(list(enumerate(similarities)), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "El código anterior realiza un preprocesamiento muy básico sobre un corpus muy pequeño, por supuesto tiene mucho para mejorar, pero la idea es sólo mostrar un poco la utilización de gensim como herramienta para hacer análisis de similitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Respecto al uso de gensim\n",
    "\n",
    "Si bien para el estudio de \"Vector Space Models\" utilizaremos algunas herramientas hechas para el curso de Stanford CS224u (e.g. _LSA_ y _TF-IDF_), la verdad es que a la hora de trabajar a nivel más industrial (o incluso académico experimental) es recomendable utilizar los modelos de `gensim` ya que proveen implementaciones más eficientes en el marco de una librería mucho más trabajada. La razón de utilizar versiones simplificadas para el curso es simplemente por motivos pedagógicos de poder entender que es lo que está ocurriendo por detrás de los modelos con código más sencillo y no tan optimizado (práctica que suele reducir legibilidad del código)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algunos recursos extra\n",
    "\n",
    "### Tutorial de Jupyter\n",
    "\n",
    "El curso se centrará en el uso de [notebooks de Jupyter](https://jupyter.org/), por lo que es importante que tengan un conocimiento básico de la herramienta. Si bien hay muchos tutoriales dando vueltas por internet, me tomé la libertad de copiar el que brindan en el curso de CS224u, puesto que cubre todos los conceptos fundamentales necesarios. Se puede acceder desde el [repositorio original de CS224u](https://github.com/cgpotts/cs224u/blob/master/jupyter_notebook_tutorial.ipynb) o bien a través de este [mismo repositorio](jupyter_notebook_tutorial.ipynb).\n",
    "\n",
    "### Tutorial de NumPy\n",
    "\n",
    "Por otra parte, este curso hace mucho uso de la herramienta [numpy](https://www.numpy.org/), que ofrece una interfaz para utilizar vectores, matrices y en general hacer [cálculo vectorizado](https://en.wikipedia.org/wiki/Vectorization_(mathematics)) con Python, lo que hace más rápidas muchas tareas. Nuevamente, hay muchos tutoriales en internet, pero me tomé la libertad de copiar el ofrecido en el curso de CS224u. Se encuentra disponible en su [versión original](https://github.com/cgpotts/cs224u/blob/master/numpy_tutorial.ipynb) y en [este repositorio](numpy_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matemática\n",
    "\n",
    "En principio trataremos de ofrecer un curso suficientemente abierto al público en general sobre minado de datos para texto. Sin embargo, ciertos conocimientos de matemática serán requeridos. Si bien se buscará explicar los fundamentos de todo, desde su definición hasta su puesta en práctica mediante Python, la verdad es que para realmente poder entender todos estos conceptos es necesario algún nivel de conocimiento de matemática universitaria. En particular, el aprendizaje automático está muy basado en conceptos de [probabilidad y estadística](https://www.khanacademy.org/math/statistics-probability), [álgebra lineal](https://www.khanacademy.org/math/linear-algebra), [cálculo diferencial](https://www.khanacademy.org/math/differential-calculus) y [cálculo multivariable](https://www.khanacademy.org/math/multivariable-calculus). \n",
    "\n",
    "Una opción es que tomen, como oyentes, algunas materias de álgebra, análisis matemático o probabilidad y estadística, de alguna de las carreras de FAMAF. Sin embargo, los links anteriores los enviarán a los tutoriales sobre matemática brindados por [Khan Academy](https://www.khanacademy.org/math) los cuales son concisos y bastante buenos para aprender lo fundamental.\n",
    "\n",
    "Esto es opcional, y se intentará guiarlos con lo más fundamental al menos, dentro del curso. Pero en caso de que quieran aprender a un nivel más detallado, es fundamental que manejen estas herramientas."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
